# 10.1.4

# 다층 퍼셉트론은 입력층 input layer 하나와 은닉층 hidden layer 이라 불리는 하나 이상의 TLU층과 마지막 출력층 output layer으로 구성된다
# 출력층을 제외하고 모든 층은 편향 뉴런을 포함하며 다음 층과 완전히 연결되어 있다

# NOTE #
# 신호는 (입력에서 출력으로) 한 방향으로만 흐른다, 따라서 이 구조는 피드포워드 신경망, feedforward neural network, FNN 에 속한다

# 은닉층을 여러 개 쌓아 올린 인공 신경망을 심층 신경망, Deep Neural Network, DNN 이라고 한다
# 딥러닝은 심층 신경망을 연구하는 분야이며 조금 더 일반적으로는 연산이 연속하여 길게 연결된 모델을 연구한다

# 연구자들을 다층 퍼셉트론을 훈련할 방법을 찾기 위해 노력 but 성공 X
# 1986년, 데이비드 루멜하트, 제프리 힌턴, 로날드 윌리엄스가 
# 역전파(backpropagation) 훈련 알고리즘을 소개하는 획기적인 논문을 공개
# 이 알고리즘은 오늘날에도 여전히 사용
# 간단히 말하면 이 알고리즘은 효율적인 기법으로 Gradient를 자동으로 계산하는 경사 하강법이다
# 네트워크를 두 번 (정방향 한 번, 역방향 한 번) 통과하는 것만으로 이 역전파 알고리즘은 
# 모든 모델 파라미터에 대한 네트워크 오차의 gradient를 계산할 수 있다
# 다른 말로 하면 오차를 감소시키기 위해 각 연결 가중치와 편향값이 어떻게 바뀌어야 할 지 알 수 있다
# Gradient를 구하고 나면 평범한 경사 하강법을 수행, 전체 과정은 네트워크가 어떤 해결책으로 수렴될 때까지 반복

# NOTE #
# 자동으로 gradient를 게산하는 것을 자동 미분이라고 부른다

# 역전파 훈련 알고리즘 더 자세히

# 한 번에 하나의 미니배치씩 진행하여 전체 훈련 세트를 처리한다
# 이 과정을 여러 번 반복하는데 각 반복을 epoch 라고 부른다 (훈련 횟수!)
# 각 미니배치는 네트워크의 입력층으로 전달되어 첫 번째 은닉층으로 보내진다
# 그 다음 미니 배치에 있는 모든 샘플에 대해 해당 층에 있는 모든 뉴런의 출력을 계산한다
# 이 결과는 다음 층으로 전달, 다시 이 층의 출력을 계산하고 결과는 다음 층으로 전달
# 이런 식으로 마지막 층인 출력츠의 출력을 게산할 때까지 계속된다
# 이것이 정방향 계산 forward pass 이다
# 역방향 계산을 위해 중간 계산값을 모두 저장하는 것 외에는 예측을 만드는 것과 정확히 같다

# 그 다음 알고리즘이 네트워크의 출력 오차를 측정한다
# 즉, 손실 함수를 사용하여 기대하는 출력과 네트워크의 실제 출력을 비교하고 오차 측정 값을 반환한다

# 이제 각 출력 연결이 이 오차에 기여하는 정도를 계산한다
# 연쇄 법칙 chain rule을 적용하면 이 단계를 빠르고 정확하게 수행할 수 있다

# 이 알고리즘은 또 다시 연쇄 법칙을 사용하여 이전 층의 연결 가중치가 이 오차의 기여 정도에
# 얼마나 기여했는지 측정한다
# 이렇게 입력층에 도달할 때까지 역방향으로 계속된다
# 이런 역방향 단계는 오차 gradient를 거꾸로 전파함으로써 효율적으로 네트워크에 있는
# 모든 연결 가중치에 대한 오차 gradient를 측정한다

# 마지막으로 알고리즘은 경사 하강법을 수행하여 방금 게산한 오차 gradient를 사용해
# 네트워크에 있는 모든 연결 가중치를 수정한다

# 요약하면,,
# 각 훈련 샘플에 대해 역전파 알고리즘이 먼저 예측을 만들고(정방향 계산) 오차를 측정한다
# 그런 다음 역방향으로 각 층을 거치면서 각 연결이 오차에 기여한 정도를 측정한다(역방향 계산)
# 마지막으로 이 오차가 감소하도록 가중치를 조정한다(경사 하강법 단계)

# 이 알고리즘을 잘 작동하고자 논문 저자들을 다층 퍼셉트론 구조에 중요한 변활르 주었다
# 계단 함수를 로지스틱(시그모이드) 함수로 바꾼 것
# 계단 함수는 수평선밖에 없으니 계산할 gradient가 없다(경사 하강법은 평편한 곳을 이동할 수 없다)
# 반면 로지스틱 함수는 어디서든지 0이 아닌 gradient가 잘 정의되어 있다
# 로지스틱 함수와 함께 널리 쓰이는 함수 : 하이퍼볼릭 탄젠트 함수(쌍곡 탄젠트 함수, tanh), ReLU함수(relu)
