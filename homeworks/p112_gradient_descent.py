# 8장 경사 하강법

# 데이터 과학을 하다보면 특정 상황에 가장 적합한 모델을 찾아야 할 때가 많다
# 여기서 가장 적합한(best) 이라 함은 대부분 모델의 오류를 최소하하는 또는 가능도를최대화 하는 것을 의미
# 어떤 최적화 문제에 관한 답을 내리는 것

# 앞으로 여러 최적화 문제를 풀어야 함을 의미
# 문제를 풀기 위해 경사 하강법이라 부르는 방법을 사용

# 8.1 경사 하강법에 숨은 의미
# 실수 vector를 입력하면 실수 하나를 출력해주는 함수 f가 있다 치자
# 이런 함수일 것이다

'''
from scratch_linear_algebra import Vector, dot

def sum_of_squares(v: Vector):
    # v에 속해 있는 항목들의 제곱합을 계산한다
    return dot(v, v)

'''

# 이런 류의 함수를 종종 최대화(최소화)해야 한다
# 즉, 함수 f를 최대화시키는 입력값 v를 찾아야 한다
# 그래디언트(gradient, 경사, 기울기)(미적분을 기억하면 이것은 편미분 벡터임을 알 수 있다)는
# 함수가 가장 빠르게 증가할 수 있는 방향을 나타냄
# 따라서 함수의 최댓값을 구하는 방법 중 하나는 임의의 시작점을 잡은 후
# 그래디언트를 계산하고 그래디언트의 방향(즉 함수의 출력값이 가장 많이 증가하는 방향)으로
# 조금 이동하는 과정을 여러 번 반복하는 것이다
# 마찬가지로 함수의 최솟값은 반대 방향으로 이동함으로써 구할 수 있다

